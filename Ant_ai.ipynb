{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNCI8Dqg1niDB3C6R8OqepG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/var-567/Ant.ai/blob/master/Ant_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preparation**\n",
        "##*Scrapping data and preprocessing*"
      ],
      "metadata": {
        "id": "wHh9_TfE-krW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o7311dxl8cQq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://gutenberg.net.au/ebooks05/0500071h.html#H5\"\n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "MJydC7g3--re"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "hkH93zrd_HBQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = soup.find_all('p')\n",
        "# paragraph variable contains the *List* of all the <p> tag contents.\n",
        "# Check if there are at least 4 <p> tags\n",
        "if len(paragraphs) >= 4:\n",
        "    #scrap all the data after the fourth para to eleminate the links and store only the text.\n",
        "    text_content = ''\n",
        "    for paragraph in paragraphs[16:]:\n",
        "      for i in paragraph.get_text():\n",
        "        if ord(i) < 128 :  #remove all the non standard English characters\n",
        "          text_content += i\n",
        "        else:\n",
        "          text_content += ''+ '\\n\\n'\n",
        "      text_content += '\\n\\n'\n",
        "    text_content = re.sub(r'^\\d+\\.', '',text_content, flags=re.MULTILINE)\n",
        "    # Write the text content to a file\n",
        "    with open('Cook_Book.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(text_content)\n",
        "else:\n",
        "    print(\"There are fewer than 4 <p> tags on the page.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AZCgaPTf_Ldc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Let's build a GPT model**\n",
        "GPT-generative pretrained transformer.\n",
        "```\n",
        "\n",
        "It is an **Autoregressive** ,*Unidirectional ,Transformer Encoder* architecture.\n",
        "\n",
        "##**Aim**\n",
        "To build a model that generate recipes that mimic the style and flavors typical of Indian cuisine\n",
        "basically to predict the next text in the sequence by context provided by the preceding words."
      ],
      "metadata": {
        "id": "mhqq_D7FI2W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find the number of unique characters in the data.\n",
        "chars=sorted(list(set(text_content)))\n",
        "vocab_size=len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n",
        "##now 72 is our vocabulary size.the possible character the model can generate."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwsiHTRuIjEN",
        "outputId": "1092bad0-991d-4a8d-94e1-3c686cce30fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \"&'(),-.0123456789:;ABCDEFGHIJKLMNOPQRSTUVWYabcdefghijklmnopqrstuvwxyz\n",
            "72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokeizing - converting text to numericals.since machines uderstand only that."
      ],
      "metadata": {
        "id": "injKXxMfPUnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#strategy to tokenize to input text,since we are building a character level language model\n",
        "\n",
        "toint={ch:i for i,ch in enumerate(chars)}#assigns a number to every letter in the character array.\n",
        "tochar={i:ch for i,ch in enumerate(chars)}\n",
        "encode=lambda s:[toint[c] for c in s]\n",
        "decode=lambda l:''.join([tochar[i] for i in l])\n",
        "# print(encode(\"hello\"))\n",
        "# print(decode(encode(\"hello\")))\n",
        "#there are various ways to do this like with tiktoken or sentence peice etc."
      ],
      "metadata": {
        "id": "4H4aTVY4Nfkf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoding the data.since a character token level model is build this a character level ecoding is done."
      ],
      "metadata": {
        "id": "8nWXCGdHBRcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data=torch.tensor(encode(text_content),dtype=torch.long)\n",
        "print(data.shape,data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "id": "evInBtHjPAew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a8f6d24e-68b5-40a9-df3c-d157f170a82d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([259911]) torch.int64\n",
            "tensor([ 0,  0, 23, 60, 54, 57, 50, 49,  1, 39, 54, 48, 50,  0,  0,  0, 44, 46,\n",
            "        64, 53,  1, 53, 46, 57, 51,  1, 46,  1, 61, 60, 66, 59, 49,  1, 60, 63,\n",
            "         1, 46,  1, 48, 60, 60, 59, 56, 50, 50, 51, 66, 57,  1, 60, 51,  1, 63,\n",
            "        54, 48, 50,  7,  1, 46, 59, 49,  1, 61, 66, 65,  1, 54, 65,  1, 65, 60,\n",
            "         1, 47, 60, 54, 57,  1, 54, 59,  0, 46,  1, 57, 46, 63, 52, 50,  1, 62,\n",
            "        66, 46, 59, 65, 54, 65, 70,  1, 60, 51,  1, 68, 46, 65, 50, 63,  7,  1,\n",
            "        60, 67, 50, 63,  1, 46,  1, 47, 63, 54, 64, 56,  1, 51, 54, 63, 50,  9,\n",
            "         1, 30, 58, 58, 50, 49, 54, 46, 65, 50, 57, 70,  1, 65, 53, 50,  1, 63,\n",
            "        54, 48, 50,  0, 47, 50, 52, 54, 59, 64,  1, 65, 60,  1, 47, 60, 54, 57,\n",
            "         7,  1, 65, 53, 50,  1, 68, 46, 65, 50, 63,  1, 68, 54, 57, 57,  1, 47,\n",
            "        66, 47, 47, 57, 50,  1, 66, 61,  1, 65, 60,  1, 65, 53, 50,  1, 64, 66,\n",
            "        63, 51, 46, 48, 50,  1, 60, 51,  1, 65, 53, 50,  1, 61, 60, 65,  0, 46,\n",
            "        59, 49,  1, 60, 67, 50, 63, 51, 57, 60, 68,  7,  1, 48, 46, 63, 63, 70,\n",
            "        54, 59, 52,  1, 46, 68, 46, 70,  1, 62, 66, 46, 59, 65, 54, 65, 54, 50,\n",
            "        64,  1, 60, 51,  1, 64, 48, 66, 58,  1, 46, 59, 49,  1, 54, 58, 61, 66,\n",
            "        63, 54, 65, 54, 50, 64,  9,  1, 41, 53, 50,  0, 48, 60, 67, 50, 63,  1,\n",
            "        60, 51,  1, 65, 53, 50,  1, 61, 60, 65,  1, 64, 53, 60, 66, 57, 49,  1,\n",
            "        59, 60, 68,  1, 47, 50,  1, 56, 50, 61, 65,  1, 61, 46, 63, 65, 54, 46,\n",
            "        57, 57, 70,  1, 60, 61, 50, 59,  7,  1, 46, 59, 49,  1, 65, 53, 50,  1,\n",
            "        63, 54, 48, 50,  0, 64, 65, 54, 63, 63, 50, 49,  1, 65, 60,  1, 61, 63,\n",
            "        50, 67, 50, 59, 65,  1, 46, 59,  1, 50, 59, 65, 54, 63, 50,  1, 60, 67,\n",
            "        50, 63, 51, 57, 60, 68,  1, 60, 51,  1, 65, 53, 50,  1, 68, 46, 65, 50,\n",
            "        63,  9,  1, 36, 59,  1, 65, 53, 50,  0, 64, 66, 47, 64, 54, 49, 54, 59,\n",
            "        52,  1, 60, 51,  1, 65, 53, 50,  1, 68, 46, 65, 50, 63,  1, 60, 63,  1,\n",
            "        65, 53, 50,  1, 47, 66, 47, 47, 57, 54, 59, 52,  7,  1, 65, 53, 50,  1,\n",
            "        51, 54, 63, 50,  1, 64, 53, 60, 66, 57, 49,  1, 47, 50,  1, 63, 50, 49,\n",
            "        66, 48, 50, 49,  7,  0, 66, 59, 65, 54, 57,  1, 54, 65,  1, 54, 64,  1,\n",
            "        64, 46, 65, 54, 64, 51, 46, 48, 65, 60, 63, 54, 57, 70,  1, 46, 64, 48,\n",
            "        50, 63, 65, 46, 54, 59, 50, 49,  1, 65, 53, 46, 65,  1, 65, 53, 50,  1,\n",
            "        52, 63, 46, 54, 59, 64,  1, 60, 51,  1, 63, 54, 48, 50,  7,  0, 68, 54,\n",
            "        65, 53, 60, 66, 65,  1, 47, 50, 54, 59, 52,  1, 61, 46, 61, 61, 70,  7,\n",
            "         1, 46, 63, 50,  1, 62, 66, 54, 65, 50,  1, 64, 60, 51, 65,  7,  1, 68,\n",
            "        53, 50, 59,  1, 65, 53, 50,  1, 61, 60, 65,  1, 64, 53, 60, 66, 57, 49,\n",
            "         1, 47, 50,  1, 63, 50, 58, 60, 67, 50, 49,  0, 51, 63, 60, 58,  1, 65,\n",
            "        53, 50,  1, 51, 54, 63, 50,  1, 46, 59, 49,  1, 46,  1, 62, 66, 46, 63,\n",
            "        65,  1, 60, 51,  1, 48, 60, 57, 49,  1, 68, 46, 65, 50, 63,  1, 47, 50,\n",
            "         1, 46, 49, 49, 50, 49,  9,  1, 22, 57, 57,  1, 65, 53, 50,  1, 57, 54,\n",
            "        62, 66, 54, 49,  7,  0, 68, 53, 54, 48, 53,  1, 54, 64,  1,  2, 48, 60,\n",
            "        59, 55, 50, 50,  7,  2,  1, 64, 53, 60, 66, 57, 49,  1, 65, 53, 50, 59,\n",
            "         1, 47, 50,  1, 49, 63, 46, 54, 59, 50, 49,  7,  1, 46, 59, 49,  1, 65,\n",
            "        53, 50,  1, 61, 60, 65,  1, 63, 50, 61, 57, 46, 48, 50, 49,  0, 60, 67,\n",
            "        50, 63,  1, 46,  1, 52, 50, 59, 65, 57, 50,  1, 48, 53, 46, 63, 48, 60,\n",
            "        46, 57,  1, 53, 50, 46, 65,  7,  1, 65, 60,  1, 46, 57, 57, 60, 68,  1,\n",
            "        46, 57, 57,  1, 58, 60, 54, 64, 65, 66, 63, 50,  1, 65, 60,  1, 50, 67,\n",
            "        46, 61, 60, 63, 46, 65, 50,  7,  0, 46, 64, 64, 54, 64, 65, 54, 59, 52,\n",
            "         1, 65, 53, 50,  1, 61, 63, 60, 48, 50, 64, 64,  1, 47, 70,  1, 60, 48,\n",
            "        48, 46, 64, 54, 60, 59, 46, 57, 57, 70,  1, 64, 53, 46, 56, 54, 59, 52,\n",
            "         1, 65, 53, 50,  1, 61, 60, 65,  7,  1, 60, 63,  1, 64, 65, 54, 63, 63,\n",
            "        54, 59, 52,  0, 54, 65, 64,  1, 48, 60, 59, 65, 50, 59, 65, 64,  1, 52,\n",
            "        50, 59, 65, 57, 70,  1, 68, 54, 65, 53,  1, 46,  1, 68, 60, 60, 49, 50,\n",
            "        59,  1, 64, 61, 60, 60, 59,  9,  1, 41, 54, 58, 50,  1, 65, 60,  1, 47,\n",
            "        60, 54, 57, 20,  1, 53, 46, 57, 51,  1, 46, 59,  0, 53, 60, 66, 63,  9,\n",
            "         0,  0, 41, 53, 50,  1, 48, 60, 60, 59, 56, 50, 50,  1, 60, 51,  1, 63,\n",
            "        54, 48, 50,  1, 68, 53, 50, 59,  1, 61, 63, 60, 61, 50, 63, 57, 70,  1,\n",
            "        47, 60, 54, 57, 50, 49,  1, 68, 54, 57, 57,  1, 51, 54, 57, 57,  1, 46,\n",
            "         1, 52, 60, 60, 49,  8, 64, 54, 71, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spliting training and testing data."
      ],
      "metadata": {
        "id": "BXPyEKZdQWSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=int(0.9*len(text_content))\n",
        "train_data=data[:n]\n",
        "val_data=data[n:]\n",
        "train_data[:1000]"
      ],
      "metadata": {
        "id": "rFEcx7t4Qcxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "5aca8a54-6888-450a-975e-12a60b92b286"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0, 23, 60, 54, 57, 50, 49,  1, 39, 54, 48, 50,  0,  0,  0, 44, 46,\n",
              "        64, 53,  1, 53, 46, 57, 51,  1, 46,  1, 61, 60, 66, 59, 49,  1, 60, 63,\n",
              "         1, 46,  1, 48, 60, 60, 59, 56, 50, 50, 51, 66, 57,  1, 60, 51,  1, 63,\n",
              "        54, 48, 50,  7,  1, 46, 59, 49,  1, 61, 66, 65,  1, 54, 65,  1, 65, 60,\n",
              "         1, 47, 60, 54, 57,  1, 54, 59,  0, 46,  1, 57, 46, 63, 52, 50,  1, 62,\n",
              "        66, 46, 59, 65, 54, 65, 70,  1, 60, 51,  1, 68, 46, 65, 50, 63,  7,  1,\n",
              "        60, 67, 50, 63,  1, 46,  1, 47, 63, 54, 64, 56,  1, 51, 54, 63, 50,  9,\n",
              "         1, 30, 58, 58, 50, 49, 54, 46, 65, 50, 57, 70,  1, 65, 53, 50,  1, 63,\n",
              "        54, 48, 50,  0, 47, 50, 52, 54, 59, 64,  1, 65, 60,  1, 47, 60, 54, 57,\n",
              "         7,  1, 65, 53, 50,  1, 68, 46, 65, 50, 63,  1, 68, 54, 57, 57,  1, 47,\n",
              "        66, 47, 47, 57, 50,  1, 66, 61,  1, 65, 60,  1, 65, 53, 50,  1, 64, 66,\n",
              "        63, 51, 46, 48, 50,  1, 60, 51,  1, 65, 53, 50,  1, 61, 60, 65,  0, 46,\n",
              "        59, 49,  1, 60, 67, 50, 63, 51, 57, 60, 68,  7,  1, 48, 46, 63, 63, 70,\n",
              "        54, 59, 52,  1, 46, 68, 46, 70,  1, 62, 66, 46, 59, 65, 54, 65, 54, 50,\n",
              "        64,  1, 60, 51,  1, 64, 48, 66, 58,  1, 46, 59, 49,  1, 54, 58, 61, 66,\n",
              "        63, 54, 65, 54, 50, 64,  9,  1, 41, 53, 50,  0, 48, 60, 67, 50, 63,  1,\n",
              "        60, 51,  1, 65, 53, 50,  1, 61, 60, 65,  1, 64, 53, 60, 66, 57, 49,  1,\n",
              "        59, 60, 68,  1, 47, 50,  1, 56, 50, 61, 65,  1, 61, 46, 63, 65, 54, 46,\n",
              "        57, 57, 70,  1, 60, 61, 50, 59,  7,  1, 46, 59, 49,  1, 65, 53, 50,  1,\n",
              "        63, 54, 48, 50,  0, 64, 65, 54, 63, 63, 50, 49,  1, 65, 60,  1, 61, 63,\n",
              "        50, 67, 50, 59, 65,  1, 46, 59,  1, 50, 59, 65, 54, 63, 50,  1, 60, 67,\n",
              "        50, 63, 51, 57, 60, 68,  1, 60, 51,  1, 65, 53, 50,  1, 68, 46, 65, 50,\n",
              "        63,  9,  1, 36, 59,  1, 65, 53, 50,  0, 64, 66, 47, 64, 54, 49, 54, 59,\n",
              "        52,  1, 60, 51,  1, 65, 53, 50,  1, 68, 46, 65, 50, 63,  1, 60, 63,  1,\n",
              "        65, 53, 50,  1, 47, 66, 47, 47, 57, 54, 59, 52,  7,  1, 65, 53, 50,  1,\n",
              "        51, 54, 63, 50,  1, 64, 53, 60, 66, 57, 49,  1, 47, 50,  1, 63, 50, 49,\n",
              "        66, 48, 50, 49,  7,  0, 66, 59, 65, 54, 57,  1, 54, 65,  1, 54, 64,  1,\n",
              "        64, 46, 65, 54, 64, 51, 46, 48, 65, 60, 63, 54, 57, 70,  1, 46, 64, 48,\n",
              "        50, 63, 65, 46, 54, 59, 50, 49,  1, 65, 53, 46, 65,  1, 65, 53, 50,  1,\n",
              "        52, 63, 46, 54, 59, 64,  1, 60, 51,  1, 63, 54, 48, 50,  7,  0, 68, 54,\n",
              "        65, 53, 60, 66, 65,  1, 47, 50, 54, 59, 52,  1, 61, 46, 61, 61, 70,  7,\n",
              "         1, 46, 63, 50,  1, 62, 66, 54, 65, 50,  1, 64, 60, 51, 65,  7,  1, 68,\n",
              "        53, 50, 59,  1, 65, 53, 50,  1, 61, 60, 65,  1, 64, 53, 60, 66, 57, 49,\n",
              "         1, 47, 50,  1, 63, 50, 58, 60, 67, 50, 49,  0, 51, 63, 60, 58,  1, 65,\n",
              "        53, 50,  1, 51, 54, 63, 50,  1, 46, 59, 49,  1, 46,  1, 62, 66, 46, 63,\n",
              "        65,  1, 60, 51,  1, 48, 60, 57, 49,  1, 68, 46, 65, 50, 63,  1, 47, 50,\n",
              "         1, 46, 49, 49, 50, 49,  9,  1, 22, 57, 57,  1, 65, 53, 50,  1, 57, 54,\n",
              "        62, 66, 54, 49,  7,  0, 68, 53, 54, 48, 53,  1, 54, 64,  1,  2, 48, 60,\n",
              "        59, 55, 50, 50,  7,  2,  1, 64, 53, 60, 66, 57, 49,  1, 65, 53, 50, 59,\n",
              "         1, 47, 50,  1, 49, 63, 46, 54, 59, 50, 49,  7,  1, 46, 59, 49,  1, 65,\n",
              "        53, 50,  1, 61, 60, 65,  1, 63, 50, 61, 57, 46, 48, 50, 49,  0, 60, 67,\n",
              "        50, 63,  1, 46,  1, 52, 50, 59, 65, 57, 50,  1, 48, 53, 46, 63, 48, 60,\n",
              "        46, 57,  1, 53, 50, 46, 65,  7,  1, 65, 60,  1, 46, 57, 57, 60, 68,  1,\n",
              "        46, 57, 57,  1, 58, 60, 54, 64, 65, 66, 63, 50,  1, 65, 60,  1, 50, 67,\n",
              "        46, 61, 60, 63, 46, 65, 50,  7,  0, 46, 64, 64, 54, 64, 65, 54, 59, 52,\n",
              "         1, 65, 53, 50,  1, 61, 63, 60, 48, 50, 64, 64,  1, 47, 70,  1, 60, 48,\n",
              "        48, 46, 64, 54, 60, 59, 46, 57, 57, 70,  1, 64, 53, 46, 56, 54, 59, 52,\n",
              "         1, 65, 53, 50,  1, 61, 60, 65,  7,  1, 60, 63,  1, 64, 65, 54, 63, 63,\n",
              "        54, 59, 52,  0, 54, 65, 64,  1, 48, 60, 59, 65, 50, 59, 65, 64,  1, 52,\n",
              "        50, 59, 65, 57, 70,  1, 68, 54, 65, 53,  1, 46,  1, 68, 60, 60, 49, 50,\n",
              "        59,  1, 64, 61, 60, 60, 59,  9,  1, 41, 54, 58, 50,  1, 65, 60,  1, 47,\n",
              "        60, 54, 57, 20,  1, 53, 46, 57, 51,  1, 46, 59,  0, 53, 60, 66, 63,  9,\n",
              "         0,  0, 41, 53, 50,  1, 48, 60, 60, 59, 56, 50, 50,  1, 60, 51,  1, 63,\n",
              "        54, 48, 50,  1, 68, 53, 50, 59,  1, 61, 63, 60, 61, 50, 63, 57, 70,  1,\n",
              "        47, 60, 54, 57, 50, 49,  1, 68, 54, 57, 57,  1, 51, 54, 57, 57,  1, 46,\n",
              "         1, 52, 60, 60, 49,  8, 64, 54, 71, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size=16\n",
        "block_size=64\n",
        "\n",
        "#--block size , batch size are Hyperparameter .\n",
        "#batch size is the number of batches that runs in parallel.\n",
        "#block size is the number of character that the character that we are going to predict gets context from.\n"
      ],
      "metadata": {
        "id": "n3gpkgFtRK4e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a utility function to get the sample batch to process.each batch of data is trained to\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "AsfqjN9mTkD-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "qNB_lGFuAC8z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A Single head of attention machanism.\n",
        "The main part where the code every character gets context from the previous words."
      ],
      "metadata": {
        "id": "Kkm0_cgRyfFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hyperparameters"
      ],
      "metadata": {
        "id": "YF-jYZ7L-_7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "jUQXRW_wzcd-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        #self is the attribute of every instance of the class.\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        #drop out layer is an regularization layer,which sets a fraction of input unit to 0 to prevent overfitting of module.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        #implemets casual masking which ensures that each position can only attend to its previous position but not the furthur ones.\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "lLO8wBv9ToIB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiple heads of self-attention in parallel"
      ],
      "metadata": {
        "id": "y8_axs6Cz7C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "      #concatination of each output from the multihead attention.\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZNhY4929ysZc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A simple linear layer followed by a non-linearity (relu) and dropout()"
      ],
      "metadata": {
        "id": "3NjHVHdw3jIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "C8vnoXOE1brD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A Combined transformer bloack implemeting all the class defined and enables communication between them."
      ],
      "metadata": {
        "id": "Fw-3KK_j6Jlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "VAJZWqT_6HQt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A Complete Model that implements multi-head self attention,add and norm,a feed forward neural network,resuidal connections."
      ],
      "metadata": {
        "id": "zqBysLkm62et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "ixfIs6ON6xIr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200"
      ],
      "metadata": {
        "id": "mxmcmqY-_O5A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create an instance of bigramLanguage Model implemented.\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "#an  optimization algorithm used to update the weights of a neural network during training.\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-MrgOyw_Jet",
        "outputId": "4dfefc7b-b444-45e7-fa6e-890ec7b490fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21268 M parameters\n",
            "step 0: train loss 4.4597, val loss 4.4671\n",
            "step 100: train loss 2.5274, val loss 2.5824\n",
            "step 200: train loss 2.4211, val loss 2.4881\n",
            "step 300: train loss 2.3739, val loss 2.4347\n",
            "step 400: train loss 2.3242, val loss 2.4082\n",
            "step 500: train loss 2.2605, val loss 2.3470\n",
            "step 600: train loss 2.1743, val loss 2.2724\n",
            "step 700: train loss 2.0824, val loss 2.2061\n",
            "step 800: train loss 1.9990, val loss 2.1503\n",
            "step 900: train loss 1.9180, val loss 2.1000\n",
            "step 1000: train loss 1.8544, val loss 2.0571\n",
            "step 1100: train loss 1.8114, val loss 2.0257\n",
            "step 1200: train loss 1.7492, val loss 1.9966\n",
            "step 1300: train loss 1.6964, val loss 1.9537\n",
            "step 1400: train loss 1.6556, val loss 1.9371\n",
            "step 1500: train loss 1.6119, val loss 1.9097\n",
            "step 1600: train loss 1.5788, val loss 1.9049\n",
            "step 1700: train loss 1.5456, val loss 1.8827\n",
            "step 1800: train loss 1.5103, val loss 1.8722\n",
            "step 1900: train loss 1.4852, val loss 1.8575\n",
            "step 2000: train loss 1.4653, val loss 1.8609\n",
            "step 2100: train loss 1.4428, val loss 1.8311\n",
            "step 2200: train loss 1.4159, val loss 1.8258\n",
            "step 2300: train loss 1.3995, val loss 1.8228\n",
            "step 2400: train loss 1.3927, val loss 1.8050\n",
            "step 2500: train loss 1.3703, val loss 1.7924\n",
            "step 2600: train loss 1.3409, val loss 1.7844\n",
            "step 2700: train loss 1.3321, val loss 1.7815\n",
            "step 2800: train loss 1.3268, val loss 1.7925\n",
            "step 2900: train loss 1.3042, val loss 1.7727\n",
            "step 3000: train loss 1.2993, val loss 1.7624\n",
            "step 3100: train loss 1.2920, val loss 1.7740\n",
            "step 3200: train loss 1.2887, val loss 1.7820\n",
            "step 3300: train loss 1.2676, val loss 1.7659\n",
            "step 3400: train loss 1.2760, val loss 1.7529\n",
            "step 3500: train loss 1.2513, val loss 1.7504\n",
            "step 3600: train loss 1.2477, val loss 1.7789\n",
            "step 3700: train loss 1.2331, val loss 1.7603\n",
            "step 3800: train loss 1.2353, val loss 1.7368\n",
            "step 3900: train loss 1.2243, val loss 1.7509\n",
            "step 4000: train loss 1.2204, val loss 1.7514\n",
            "step 4100: train loss 1.2114, val loss 1.7374\n",
            "step 4200: train loss 1.2064, val loss 1.7653\n",
            "step 4300: train loss 1.1892, val loss 1.7333\n",
            "step 4400: train loss 1.1831, val loss 1.7299\n",
            "step 4500: train loss 1.1912, val loss 1.7452\n",
            "step 4600: train loss 1.1717, val loss 1.7426\n",
            "step 4700: train loss 1.1717, val loss 1.7275\n",
            "step 4800: train loss 1.1672, val loss 1.6985\n",
            "step 4900: train loss 1.1733, val loss 1.7293\n",
            "step 4999: train loss 1.1608, val loss 1.7359\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Straw Halls\n",
            "\n",
            "\n",
            "Put until of the stuffs of the\n",
            "skin of cardamoms cut milks and\n",
            "water, a fie-tender put tincturs of saside a\n",
            "ghter, mastely over freeze enger freeped or five quortered over the\n",
            "tode the top of finely-chaviulared well to or until the lie-part of and\n",
            "syrup, cork over dished half made; hal, of part it the prove vinegar up\n",
            "a quarter, and add the time, with in a little of gullet currough\n",
            "are ounce-bowne ghee-sized si\"ixty-to preserve No. 27nive to be of durred gradually,\n",
            "and remove dozen ground chiled cold, them filter to alto, which\n",
            "with eight sugar of the and meately, of pour ten or its larder legar from a shapee other\n",
            "lays; and add to of the melted chees, for milks\n",
            "other used quanties of ground cough, and add the water tyre quarts of water pickles,\n",
            "one spoon of the congeree of sted eely-set boiled to as strained bef\n",
            "thine boiled spices; flour; then stir up hot; of from finely-grass well-beat bef souped. The\n",
            "sted being throw in the weater, with stould tendy-wer, dipe and a half of\n",
            "esse-flour, tumples, taking the\n",
            "drops of the deel hears, and steep with the meat, of the cuttonees\n",
            "boile the poundered of cloves, salt well-water, and p; boil well to\n",
            "of the rose, three dish when horeze by the white spoonately a\n",
            "pound sugar to fillet a rupee: the smel, feweds the meather of pouces\n",
            "chahkeesed Dal them over six large brown the oces and stand and through\n",
            "and a liusly cheesegs or chittacks of a slow fire, five it in\n",
            "smalade finely recipe.\n",
            "\n",
            "\n",
            "\n",
            "Sauce Cintred\n",
            "Cheesee tender\n",
            "\n",
            "\n",
            "Take cooled served them in two one-ing other dropoes of very\n",
            "doopian white of an equal\n",
            "trable out it tender to the dock dish, or itsee\n",
            "teaspees, and plate a part of a teaspoonful of\n",
            "Waray, carefuls, or and make the the\n",
            "proth; or all the respoonful of a time, to being strew all hour it to dripe it with\n",
            "white well-sized with a chickened epredings hot chittacks or stewers, put the\n",
            "glass neced, stirred of Chutnee Aset and well tenders and pie-quarty; then\n",
            "after the tender leaf.\n",
            "\n",
            "\n",
            "\n",
            "Baranged\n",
            "Cheesee\n",
            "\n"
          ]
        }
      ]
    }
  ]
}